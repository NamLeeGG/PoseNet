# @package _global_

# Example hyperparameter optimization using Optuna:
# Run: python train.py -m hparams_search=optuna

defaults:
  - override /hydra/sweeper: optuna

# Choose metric to be optimized by Optuna
# Ensure this matches a metric logged in your LightningModule!
optimized_metric: "val/nme"  # Normalized Mean Error (lower is better)

hydra:
  mode: "MULTIRUN" # Enables hyperparameter sweeps

  sweeper:
    _target_: hydra_plugins.hydra_optuna_sweeper.optuna_sweeper.OptunaSweeper

    # Storage URL to persist optimization results (optional)
    storage: null

    # Study name (optional)
    study_name: "posenet_hyperopt"

    # Number of parallel workers (1 GPU available)
    n_jobs: 1

    # Objective direction: 'minimize' (for NME)
    direction: minimize

    # Total number of trials
    n_trials: 30  # Increase for better optimization

    # Bayesian sampler (Tree-structured Parzen Estimator)
    sampler:
      _target_: optuna.samplers.TPESampler
      seed: 1234
      n_startup_trials: 10 # Use random sampling for first 10 trials

    # Define hyperparameter search space
    params:
      model.optimizer.lr: interval(0.0001, 0.01)  # Learning rate tuning
      model.optimizer.weight_decay: interval(1e-6, 1e-4)  # Regularization
      model.optimizer._target_: choice(torch.optim.Adam, torch.optim.AdamW, torch.optim.SGD)
      data.train_batch_size: choice(8, 16, 32)  # Lower batch sizes due to RTX 3060 12GB
      model.net.hidden_size: choice(128, 256, 512)  # Adjust model capacity
      model.net.dropout: interval(0.1, 0.5)  # Regularization tuning
      model.scheduler.factor: interval(0.1, 0.5)  # LR scheduler reduction factor
      model.scheduler.patience: choice(3, 5, 7)  # Early stopping patience
